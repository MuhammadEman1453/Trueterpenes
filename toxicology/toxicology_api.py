"""
Fully Optimized Enhanced Toxicology Research API with Concurrent Processing and S3 Storage
API that leverages both Google Scholar and Europe PMC with OpenAI GPT-4.1-mini filtering and automatic S3 paper storage
Features: Concurrent scraping, concurrent AI processing, exponential backoff, optimized for AWS Lambda, S3 file storage
ðŸ“‹ System Overview
This API provides a comprehensive solution for toxicology research paper discovery, analysis, and storage. It combines multiple data sources, AI-powered filtering, and automatic cloud storage to streamline toxicology research workflows.

ðŸ—ï¸ Architecture Flow
User Request â†’ Compound Validation â†’ Parallel Scraping â†’ Content Extraction â†’ AI Filtering â†’ AI Review â†’ S3 Storage â†’ Response

ðŸ“ File Documentation
1. main.py - Main API Application
Purpose: FastAPI application that orchestrates the entire toxicology research pipeline
Key Features:

RESTful API endpoints for compound searches
Concurrent processing coordination
S3 integration for automatic paper storage
Comprehensive error handling and performance monitoring

Main Flow:

Input Validation: Validates compound name/CAS number
Compound Enrichment: Fetches synonyms from PubChem
Parallel Search: Searches Google Scholar and Europe PMC simultaneously
Content Extraction: Concurrently extracts full text from PDFs and HTML
AI Filtering: Filters papers for toxicology relevance using OpenAI
AI Review: Performs detailed toxicological analysis
S3 Storage: Saves papers with metadata to AWS S3
Response: Returns structured results with performance metrics

Key Endpoints:

POST /search - Main search endpoint with full pipeline
GET /health - System health with component status
GET /test/{compound_name} - Quick testing with parameters
GET /performance-test/{compound_name} - Benchmark performance
GET /s3/list-papers - List stored papers in S3
ðŸš€ Performance Optimizations
Concurrent Processing

Scraping: 15 concurrent workers for both Google Scholar and Europe PMC
Filtering: 15 concurrent workers for OpenAI filtering
Review: 10 concurrent workers for detailed analysis
S3 Upload: 5 concurrent workers to prevent file locking

Error Handling

Exponential Backoff: All external API calls use retry logic
Graceful Degradation: Continues processing even if some components fail
Comprehensive Logging: Detailed logs for debugging and monitoring

Memory Optimization

Text Limiting: Limits content size for AI processing
Batch Processing: Processes articles in optimized batches
Async I/O: Non-blocking operations throughout

ðŸ“Š Performance Metrics
Typical Performance:

Search Phase: 10-30 seconds
Extraction Phase: 30-60 seconds
Filtering Phase: 20-40 seconds
Review Phase: 30-60 seconds
Total: 2-3 minutes for 100+ articles

Speedup: 15x-25x faster than sequential processing
"""

from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Literal
import asyncio
import logging
from datetime import datetime
import json
import re
import os
import time
import tempfile
import boto3
from botocore.exceptions import ClientError

# Import our optimized modules
from compound_validator import CompoundInput, process_compound_input
from google_scholar_scraper import OptimizedGoogleScholarScraper as GoogleScholarScraper, CompoundInfo, SearchResult
from pubmed_scraper import OptimizedEuropePMCScraper as EuropePMCScraper
from config import get_apify_token, DEFAULT_MAX_RESULTS_PER_QUERY
from article_filter import OptimizedArticleFilter as ArticleFilter
from article_reviewer import OptimizedArticleReviewer as ArticleReviewer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Fully Optimized Enhanced Toxicology Research API with S3 Storage",
    description="High-performance API for searching toxicology papers using Google Scholar and Europe PMC with concurrent processing, OpenAI GPT-4.1-mini filtering, and automatic S3 storage. Optimized for AWS Lambda deployment.",
    version="7.1.0"
)

# S3 Paper Downloader Class
class S3PaperDownloader:
    """
    Enhanced S3 downloader for toxicology research papers with full metadata storage
    """
    
    def __init__(self, 
                 aws_access_key_id: str = None,
                 aws_secret_access_key: str = None,
                 bucket_name: str = None,
                 region_name: str = 'us-east-1'):
        """
        Initialize S3 client with credentials
        """
        # Get credentials from parameters or environment variables
        self.aws_access_key_id = aws_access_key_id or os.getenv('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = aws_secret_access_key or os.getenv('AWS_SECRET_ACCESS_KEY')
        self.bucket_name = bucket_name or os.getenv('S3_BUCKET_NAME', 'toxicology-research-papers')
        self.region_name = region_name
        
        # Initialize semaphore to limit concurrent S3 operations (prevents file locking issues)
        self._upload_semaphore = asyncio.Semaphore(5)  # Max 5 concurrent uploads
        
        # Validate required parameters
        if not all([self.aws_access_key_id, self.aws_secret_access_key]):
            logger.warning("S3 credentials not found. S3 functionality will be disabled.")
            self.s3_client = None
            return
        
        # Initialize S3 client
        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.region_name
            )
            
            # Test connection and create bucket if it doesn't exist
            try:
                self.s3_client.head_bucket(Bucket=self.bucket_name)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    # Bucket doesn't exist, create it
                    self.s3_client.create_bucket(Bucket=self.bucket_name)
                    logger.info(f"Created S3 bucket: {self.bucket_name}")
                else:
                    raise
            
            logger.info(f"Successfully connected to S3 bucket: {self.bucket_name}")
            
        except ClientError as e:
            logger.error(f"Failed to connect to S3: {e}")
            self.s3_client = None
    
    def sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for S3 storage and handle Unicode characters"""
        import unicodedata
        
        # Normalize Unicode characters (convert accented chars to ASCII equivalents)
        filename = unicodedata.normalize('NFKD', filename)
        
        # Replace problem characters
        filename = re.sub(r'[<>:"/\\|?*\u0000-\u001f\u007f-\u009f]', '_', filename)
        
        # Replace Greek letters and special characters with ASCII equivalents
        replacements = {
            'Î±': 'alpha', 'Î²': 'beta', 'Î³': 'gamma', 'Î´': 'delta', 'Îµ': 'epsilon',
            'Î¶': 'zeta', 'Î·': 'eta', 'Î¸': 'theta', 'Î¹': 'iota', 'Îº': 'kappa',
            'Î»': 'lambda', 'Î¼': 'mu', 'Î½': 'nu', 'Î¾': 'xi', 'Î¿': 'omicron',
            'Ï€': 'pi', 'Ï': 'rho', 'Ïƒ': 'sigma', 'Ï„': 'tau', 'Ï…': 'upsilon',
            'Ï†': 'phi', 'Ï‡': 'chi', 'Ïˆ': 'psi', 'Ï‰': 'omega',
            'Î‘': 'Alpha', 'Î’': 'Beta', 'Î“': 'Gamma', 'Î”': 'Delta', 'Î•': 'Epsilon',
            'Î–': 'Zeta', 'Î—': 'Eta', 'Î˜': 'Theta', 'Î™': 'Iota', 'Îš': 'Kappa',
            'Î›': 'Lambda', 'Îœ': 'Mu', 'Î': 'Nu', 'Îž': 'Xi', 'ÎŸ': 'Omicron',
            'Î ': 'Pi', 'Î¡': 'Rho', 'Î£': 'Sigma', 'Î¤': 'Tau', 'Î¥': 'Upsilon',
            'Î¦': 'Phi', 'Î§': 'Chi', 'Î¨': 'Psi', 'Î©': 'Omega',
            'â„ƒ': 'C', 'Â°': 'deg', 'Â±': 'plus_minus', 'Ã—': 'x', 'Ã·': 'div',
            'â€“': '-', 'â€”': '-', '"':'"', '"': '"', '"': '"', 'â€¦': '...' 
        }
        
        for unicode_char, ascii_equiv in replacements.items():
            filename = filename.replace(unicode_char, ascii_equiv)
        
        # Remove any remaining non-ASCII characters
        filename = filename.encode('ascii', 'ignore').decode('ascii')
        
        # Replace multiple spaces/underscores with single underscore
        filename = re.sub(r'[\s_]+', '_', filename)
        
        # Remove leading/trailing underscores and dots
        filename = filename.strip('._')
        
        # Ensure filename is not too long
        if len(filename) > 100:
            filename = filename[:100]
        
        # Ensure filename is not empty
        if not filename:
            filename = "untitled"
        
        return filename
    
    def generate_paper_metadata(self, paper: Dict, compound_info: Dict, 
                              filtering_summary: Dict = None,
                              toxicology_review_summary: Dict = None) -> Dict:
        """Generate comprehensive metadata for a research paper"""
        metadata = {
            "paper_metadata": {
                "title": paper.get("title", ""),
                "authors": paper.get("authors", []),
                "abstract": paper.get("abstract", ""),
                "publication_year": paper.get("year", ""),
                "citation_count": paper.get("citation_count", 0),
                "source": paper.get("source", ""),
                "link": paper.get("link", ""),
                "pdf_link": paper.get("pdf_link", ""),
                "doi": paper.get("doi", ""),
                "search_term": paper.get("search_term", "")
            },
            "compound_information": {
                "name": compound_info.get("name", ""),
                "cas_number": compound_info.get("cas_number", ""),
                "synonyms": compound_info.get("synonyms", []),
                "iupac_name": compound_info.get("iupac_name", ""),
                "molecular_formula": compound_info.get("molecular_formula", ""),
                "molecular_weight": compound_info.get("molecular_weight", ""),
                "pubchem_cid": compound_info.get("pubchem_cid", "")
            },
            "content_extraction": {
                "full_text_available": paper.get("content_extracted", False),
                "extraction_method": paper.get("extraction_method", ""),
                "extraction_scraper": paper.get("extraction_scraper", ""),
                "text_length": paper.get("text_length", 0),
                "full_text": paper.get("full_text", "")
            },
            "ai_filtering_results": {
                "filtering_confidence": paper.get("filtering_confidence", ""),
                "study_type": paper.get("study_type", ""),
                "relevant_keywords": paper.get("relevant_keywords", []),
                "filtering_reasoning": paper.get("filtering_reasoning", ""),
                "included_in_final_results": True,
                "filtering_summary": filtering_summary
            },
            "toxicology_review": paper.get("toxicology_review", {}),
            "toxicology_endpoints": paper.get("toxicology_endpoints", {}),
            "study_design": paper.get("study_design", {}),
            "key_findings": paper.get("key_findings", ""),
            "download_metadata": {
                "download_timestamp": datetime.now().isoformat(),
                "api_version": "7.1.0",
                "processing_notes": "Automated download from Toxicology Research API"
            }
        }
        
        return metadata
    
    def create_paper_markdown(self, metadata: Dict) -> str:
        """Create a formatted markdown document from paper metadata"""
        md_content = []
        
        # Header
        md_content.append(f"# {metadata['paper_metadata']['title']}")
        md_content.append("")
        
        # Basic Information
        md_content.append("## Paper Information")
        md_content.append(f"**Authors:** {', '.join(metadata['paper_metadata']['authors'])}")
        md_content.append(f"**Publication Year:** {metadata['paper_metadata']['publication_year']}")
        md_content.append(f"**Citation Count:** {metadata['paper_metadata']['citation_count']}")
        md_content.append(f"**Source:** {metadata['paper_metadata']['source']}")
        md_content.append(f"**DOI:** {metadata['paper_metadata']['doi']}")
        md_content.append(f"**Link:** [{metadata['paper_metadata']['link']}]({metadata['paper_metadata']['link']})")
        if metadata['paper_metadata']['pdf_link']:
            md_content.append(f"**PDF Link:** [{metadata['paper_metadata']['pdf_link']}]({metadata['paper_metadata']['pdf_link']})")
        md_content.append("")
        
        # Abstract
        md_content.append("## Abstract")
        md_content.append(metadata['paper_metadata']['abstract'])
        md_content.append("")
        
        # Compound Information
        md_content.append("## Compound Information")
        md_content.append(f"**Name:** {metadata['compound_information']['name']}")
        md_content.append(f"**CAS Number:** {metadata['compound_information']['cas_number']}")
        md_content.append(f"**IUPAC Name:** {metadata['compound_information']['iupac_name']}")
        md_content.append(f"**Molecular Formula:** {metadata['compound_information']['molecular_formula']}")
        md_content.append(f"**Molecular Weight:** {metadata['compound_information']['molecular_weight']}")
        md_content.append(f"**PubChem CID:** {metadata['compound_information']['pubchem_cid']}")
        md_content.append(f"**Synonyms:** {', '.join(metadata['compound_information']['synonyms'])}")
        md_content.append("")
        
        # Content Extraction
        md_content.append("## Content Extraction")
        md_content.append(f"**Full Text Available:** {metadata['content_extraction']['full_text_available']}")
        md_content.append(f"**Extraction Method:** {metadata['content_extraction']['extraction_method']}")
        md_content.append(f"**Extraction Scraper:** {metadata['content_extraction']['extraction_scraper']}")
        md_content.append(f"**Text Length:** {metadata['content_extraction']['text_length']} characters")
        md_content.append("")
        
        # AI Filtering Results
        md_content.append("## AI Filtering Results")
        md_content.append(f"**Filtering Confidence:** {metadata['ai_filtering_results']['filtering_confidence']}")
        md_content.append(f"**Study Type:** {metadata['ai_filtering_results']['study_type']}")
        md_content.append(f"**Relevant Keywords:** {', '.join(metadata['ai_filtering_results']['relevant_keywords'])}")
        md_content.append("**Filtering Reasoning:**")
        md_content.append(metadata['ai_filtering_results']['filtering_reasoning'])
        md_content.append("")
        
        # Toxicology Review
        if metadata['toxicology_review']:
            md_content.append("## Toxicology Review")
            
            # Access Analysis
            if 'access_analysis' in metadata['toxicology_review']:
                access = metadata['toxicology_review']['access_analysis']
                md_content.append("### Access Analysis")
                md_content.append(f"**Full Paper Access:** {access.get('full_paper_access', 'N/A')}")
                md_content.append(f"**Paywall Status:** {access.get('paywall_status', 'N/A')}")
                md_content.append(f"**Content Completeness:** {access.get('content_completeness', 'N/A')}")
                md_content.append("")
            
            # Key Findings
            if 'key_findings' in metadata['toxicology_review']:
                findings = metadata['toxicology_review']['key_findings']
                md_content.append("### Key Findings")
                md_content.append(f"**Main Findings:** {findings.get('main_findings', 'N/A')}")
                md_content.append(f"**Significance:** {findings.get('significance', 'N/A')}")
                md_content.append("")
            
            # Study Summary
            if 'study_summary' in metadata['toxicology_review']:
                summary = metadata['toxicology_review']['study_summary']
                md_content.append("### Study Summary")
                md_content.append(f"**Objective:** {summary.get('objective', 'N/A')}")
                md_content.append("**Methods Brief:**")
                md_content.append(summary.get('methods_brief', 'N/A'))
                md_content.append(f"**Conclusion:** {summary.get('conclusion', 'N/A')}")
                md_content.append("")
            
            # Toxicology Endpoints
            if 'toxicology_endpoints' in metadata['toxicology_review']:
                endpoints = metadata['toxicology_review']['toxicology_endpoints']
                md_content.append("### Toxicology Endpoints")
                md_content.append(f"**EC50:** {endpoints.get('ec50', 'Not available')}")
                md_content.append(f"**NOAEL:** {endpoints.get('noael', 'Not available')}")
                md_content.append(f"**NOEL:** {endpoints.get('noel', 'Not available')}")
                md_content.append(f"**LOEAL:** {endpoints.get('loeal', 'Not available')}")
                md_content.append(f"**LOAEL:** {endpoints.get('loael', 'Not available')}")
                md_content.append(f"**LD50:** {endpoints.get('ld50', 'Not available')}")
                md_content.append("")
            
            # Study Design
            if 'study_design' in metadata['toxicology_review']:
                design = metadata['toxicology_review']['study_design']
                md_content.append("### Study Design")
                md_content.append(f"**Species:** {design.get('species', 'N/A')}")
                md_content.append(f"**Cell Type:** {design.get('cell_type', 'N/A')}")
                md_content.append(f"**Route of Exposure:** {design.get('route_of_exposure', 'N/A')}")
                md_content.append(f"**Duration:** {design.get('duration', 'N/A')}")
                md_content.append(f"**Highest Concentration:** {design.get('highest_concentration', 'N/A')}")
                md_content.append(f"**Test System:** {design.get('test_system', 'N/A')}")
                md_content.append("")
        
        # Full Text (if available)
        if metadata['content_extraction']['full_text_available'] and metadata['content_extraction']['full_text']:
            md_content.append("## Full Text")
            md_content.append("```")
            md_content.append(metadata['content_extraction']['full_text'][:5000])  # Limit to first 5000 chars
            if len(metadata['content_extraction']['full_text']) > 5000:
                md_content.append("... [Truncated for display]")
            md_content.append("```")
            md_content.append("")
        
        # Download Metadata
        md_content.append("## Download Information")
        md_content.append(f"**Downloaded:** {metadata['download_metadata']['download_timestamp']}")
        md_content.append(f"**API Version:** {metadata['download_metadata']['api_version']}")
        md_content.append(f"**Notes:** {metadata['download_metadata']['processing_notes']}")
        
        return "\n".join(md_content)
    

    async def save_paper_to_s3(self, paper: Dict, compound_info: Dict,
                               filtering_summary: Dict = None,
                               toxicology_review_summary: Dict = None) -> Dict:
        """Save individual paper with full metadata to S3"""
        if not self.s3_client:
            return {
                "success": False,
                "error": "S3 client not initialized",
                "paper_title": paper.get('title', 'Unknown'),
                "compound_name": compound_info.get('name', 'Unknown')
            }
        
        # Use semaphore to limit concurrent uploads
        async with self._upload_semaphore:
            try:
                # Generate comprehensive metadata
                metadata = self.generate_paper_metadata(
                    paper, compound_info, filtering_summary, toxicology_review_summary
                )
                
                # Create markdown content
                markdown_content = self.create_paper_markdown(metadata)
                
                # Create S3 key with better sanitization
                safe_compound = self.sanitize_filename(compound_info['name'])
                safe_title = self.sanitize_filename(paper['title'])
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")  # Add microseconds for uniqueness
                s3_key = f"toxicology_research/{safe_compound}/{timestamp}_{safe_title}.md"
                
                # Upload markdown directly to S3 without temporary file
                try:
                    # Ensure UTF-8 encoding for markdown content
                    markdown_bytes = markdown_content.encode('utf-8')
                    
                    self.s3_client.put_object(
                        Bucket=self.bucket_name,
                        Key=s3_key,
                        Body=markdown_bytes,
                        ContentType='text/markdown; charset=utf-8',
                        Metadata={
                            'compound_name': safe_compound[:100],  # Limit metadata size
                            'paper_title': safe_title[:100],
                            'upload_timestamp': datetime.now().isoformat(),
                            'encoding': 'utf-8'
                        }
                    )
                except Exception as e:
                    logger.error(f"Failed to upload markdown to S3: {e}")
                    raise
                
                # Also save JSON metadata
                json_key = s3_key.replace('.md', '.json')
                try:
                    # Ensure JSON is properly encoded as UTF-8
                    json_content = json.dumps(metadata, indent=2, ensure_ascii=False)
                    json_bytes = json_content.encode('utf-8')
                    
                    self.s3_client.put_object(
                        Bucket=self.bucket_name,
                        Key=json_key,
                        Body=json_bytes,
                        ContentType='application/json; charset=utf-8',
                        Metadata={
                            'compound_name': safe_compound[:100],
                            'paper_title': safe_title[:100],
                            'upload_timestamp': datetime.now().isoformat(),
                            'encoding': 'utf-8'
                        }
                    )
                except Exception as e:
                    logger.error(f"Failed to upload JSON to S3: {e}")
                    # Don't fail the whole operation if JSON fails
                    json_key = None
                
                logger.info(f"Successfully saved paper to S3: {s3_key}")
                
                return {
                    "success": True,
                    "s3_markdown_key": s3_key,
                    "s3_json_key": json_key,
                    "s3_url": f"s3://{self.bucket_name}/{s3_key}",
                    "paper_title": paper['title'],
                    "compound_name": compound_info['name']
                }
                
            except Exception as e:
                logger.error(f"Failed to save paper to S3: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "paper_title": paper.get('title', 'Unknown'),
                    "compound_name": compound_info.get('name', 'Unknown')
                }
    
    async def save_all_papers_to_s3(self, api_response: Dict) -> Dict:
        """Save all papers from API response to S3 with full metadata"""
        if not self.s3_client:
            return {
                "success": False,
                "error": "S3 client not initialized",
                "total_papers": len(api_response.get('search_results', [])),
                "successful_saves": 0,
                "failed_saves": len(api_response.get('search_results', []))
            }
        
        results = {
            "total_papers": len(api_response.get('search_results', [])),
            "successful_saves": 0,
            "failed_saves": 0,
            "save_details": [],
            "compound_info": api_response.get('compound_info', {}),
            "s3_bucket": self.bucket_name,
            "save_timestamp": datetime.now().isoformat()
        }
        
        # Extract common metadata
        compound_info = api_response.get('compound_info', {})
        filtering_summary = api_response.get('filtering_summary', {})
        toxicology_review_summary = api_response.get('toxicology_review_summary', {})
        
        # Save each paper
        save_tasks = []
        for paper in api_response.get('search_results', []):
            task = self.save_paper_to_s3(
                paper, 
                compound_info, 
                filtering_summary, 
                toxicology_review_summary
            )
            save_tasks.append(task)
        
        # Execute all save tasks concurrently
        if save_tasks:
            save_results = await asyncio.gather(*save_tasks, return_exceptions=True)
            
            # Process results
            for result in save_results:
                if isinstance(result, Exception):
                    # Handle exceptions from tasks
                    error_result = {
                        "success": False,
                        "error": str(result),
                        "paper_title": "Unknown",
                        "compound_name": compound_info.get('name', 'Unknown')
                    }
                    results['save_details'].append(error_result)
                    results['failed_saves'] += 1
                else:
                    results['save_details'].append(result)
                    if result['success']:
                        results['successful_saves'] += 1
                    else:
                        results['failed_saves'] += 1
        
        # Create summary report and save to S3
        safe_compound = self.sanitize_filename(compound_info.get('name', 'unknown'))
        summary_key = f"toxicology_research/{safe_compound}/batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            # Use the semaphore for summary upload too
            async with self._upload_semaphore:
                summary_content = json.dumps(results, indent=2, ensure_ascii=False)
                summary_bytes = summary_content.encode('utf-8')
                
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=summary_key,
                    Body=summary_bytes,
                    ContentType='application/json; charset=utf-8',
                    Metadata={
                        'compound_name': safe_compound[:100],
                        'total_papers': str(results['total_papers']),
                        'successful_saves': str(results['successful_saves']),
                        'batch_timestamp': datetime.now().isoformat(),
                        'encoding': 'utf-8'
                    }
                )
                results['summary_s3_key'] = summary_key
        except Exception as e:
            logger.error(f"Failed to save batch summary: {e}")
        
        logger.info(f"Batch save complete: {results['successful_saves']}/{results['total_papers']} papers saved successfully")
        
        return results

    def list_saved_papers(self, compound_name: str = None) -> List[Dict]:
        """List all saved papers in S3, optionally filtered by compound"""
        if not self.s3_client:
            return []
        
        try:
            prefix = "toxicology_research/"
            if compound_name:
                safe_compound = self.sanitize_filename(compound_name)
                prefix = f"toxicology_research/{safe_compound}/"
            
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=prefix
            )
            
            papers = []
            for obj in response.get('Contents', []):
                if obj['Key'].endswith('.md'):
                    # Get object metadata
                    metadata_response = self.s3_client.head_object(
                        Bucket=self.bucket_name,
                        Key=obj['Key']
                    )
                    
                    papers.append({
                        "s3_key": obj['Key'],
                        "size": obj['Size'],
                        "last_modified": obj['LastModified'].isoformat(),
                        "compound_name": metadata_response.get('Metadata', {}).get('compound_name', ''),
                        "paper_title": metadata_response.get('Metadata', {}).get('paper_title', ''),
                        "s3_url": f"s3://{self.bucket_name}/{obj['Key']}"
                    })
            
            return papers
        
        except Exception as e:
            logger.error(f"Failed to list saved papers: {e}")
            return []

    def get_paper_content(self, s3_key: str) -> Optional[str]:
        """Retrieve paper content from S3"""
        if not self.s3_client:
            return None
        
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            
            content = response['Body'].read().decode('utf-8')
            return content
        
        except Exception as e:
            logger.error(f"Failed to retrieve paper content: {e}")
            return None

# Extended SearchResult model
class SearchResultModel(BaseModel):
    """Individual search result model with DOI support, filtering info, and toxicology review"""
    title: str
    authors: List[str]
    abstract: str
    link: str
    pdf_link: Optional[str] = None
    citation_count: int
    year: int
    search_term: str
    full_text: Optional[str] = None
    content_extracted: bool = False
    extraction_method: Optional[str] = None
    extraction_scraper: Optional[str] = None
    text_length: int = 0
    source: str = "google_scholar"
    doi: Optional[str] = None
    
    # Filtering fields
    filtering_confidence: Optional[float] = None
    study_type: Optional[str] = None
    relevant_keywords: Optional[List[str]] = None
    filtering_reasoning: Optional[str] = None
    
    # Toxicology review fields
    toxicology_review: Optional[Dict] = None
    review_summary: Optional[str] = None
    toxicology_endpoints: Optional[Dict] = None
    study_design: Optional[Dict] = None
    
    # S3 fields
    s3_markdown_url: Optional[str] = None
    s3_json_url: Optional[str] = None
    
    class Config:
        schema_extra = {
            "example": {
                "title": "Limonene inhalation toxicity in rats",
                "authors": ["Smith J", "Doe A"],
                "abstract": "This study investigates...",
                "link": "https://example.com/paper",
                "doi": "10.1234/example.doi",
                "source": "europe_pmc",
                "filtering_confidence": 0.95,
                "study_type": "animal_studies",
                "relevant_keywords": ["inhalation", "toxicity", "gavage"],
                "toxicology_review": {
                    "key_findings": "NOAEL of 100 mg/kg/day established",
                    "study_design": {"species": "rat", "route": "inhalation"}
                },
                "s3_markdown_url": "s3://bucket/toxicology_research/limonene/20250515_paper.md",
                "s3_json_url": "s3://bucket/toxicology_research/limonene/20250515_paper.json"
            }
        }

# Request models
class CompoundRequest(BaseModel):
    """Request model for compound search with all optimization options"""
    compound_name: Optional[str] = Field(None, description="Name of the compound")
    cas_number: Optional[str] = Field(None, description="CAS registry number")
    search_sources: Optional[List[str]] = Field(["google_scholar", "europe_pmc"], description="Data sources to search")
    max_results_per_query: Optional[int] = Field(DEFAULT_MAX_RESULTS_PER_QUERY, description="Maximum results per query")
    
    # AI Processing options
    enable_filtering: Optional[bool] = Field(True, description="Enable OpenAI-based article filtering")
    min_confidence: Optional[float] = Field(0.7, description="Minimum confidence threshold for filtering (0.0-1.0)")
    enable_toxicology_review: Optional[bool] = Field(True, description="Enable comprehensive toxicology review")
    
    # S3 Storage options
    save_to_s3: Optional[bool] = Field(True, description="Save papers to S3 bucket")
    s3_folder_override: Optional[str] = Field(None, description="Override S3 folder name (default: compound name)")
    
    # Concurrent processing parameters
    filter_max_workers: Optional[int] = Field(15, description="Maximum concurrent workers for filtering")
    review_max_workers: Optional[int] = Field(10, description="Maximum concurrent workers for review")
    google_scholar_workers: Optional[int] = Field(15, description="Concurrent workers for Google Scholar extraction")
    europe_pmc_workers: Optional[int] = Field(15, description="Concurrent workers for Europe PMC extraction")
    scraper_max_workers: Optional[int] = Field(15, description="General scraper concurrency setting")
    
    class Config:
        schema_extra = {
            "example": {
                "compound_name": "limonene",
                "cas_number": "5989-27-5",
                "search_sources": ["google_scholar", "europe_pmc"],
                "max_results_per_query": 10,
                "enable_filtering": True,
                "min_confidence": 0.7,
                "enable_toxicology_review": True,
                "save_to_s3": True,
                "filter_max_workers": 15,
                "review_max_workers": 10,
                "google_scholar_workers": 15,
                "europe_pmc_workers": 15,
                "scraper_max_workers": 15
            }
        }

class ExcludedArticle(BaseModel):
    """Model for excluded articles"""
    title: str
    link: str
    exclusion_reasons: List[str]
    filtering_confidence: float
    reasoning: str

class CompoundResponse(BaseModel):
    """Response model for compound search with complete optimization metrics and S3 results"""
    success: bool
    compound_info: Dict
    search_results: List[SearchResultModel]
    excluded_articles: Optional[List[ExcludedArticle]] = None
    filtering_summary: Optional[Dict] = None
    toxicology_review_summary: Optional[Dict] = None
    statistics: Dict
    execution_time: float
    timestamp: str
    sources_searched: List[str]
    performance_metrics: Optional[Dict] = None
    s3_save_results: Optional[Dict] = None

# Initialize scrapers, filter, reviewer, and S3 downloader
google_scholar_scraper = None
europe_pmc_scraper = None
article_filter = None
article_reviewer = None
s3_downloader = None

apify_token = get_apify_token()
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize scrapers
if apify_token:
    google_scholar_scraper = GoogleScholarScraper(apify_token, max_workers=15)
    europe_pmc_scraper = EuropePMCScraper(apify_token, max_workers=15)
    logger.info("Optimized scrapers initialized successfully with Apify token")
    logger.info(f"Google Scholar scraper: {15} workers")
    logger.info(f"Europe PMC scraper: {15} workers")
else:
    logger.warning("Apify token not configured. API will return error on search requests.")

# Initialize AI components
if openai_api_key:
    article_filter = ArticleFilter(openai_api_key, max_workers=15)
    article_reviewer = ArticleReviewer(openai_api_key, max_workers=10)
    logger.info("Optimized article filter and reviewer initialized with OpenAI GPT-4.1-mini")
    logger.info(f"Article filter: {15} workers")
    logger.info(f"Article reviewer: {10} workers")
else:
    logger.warning("OpenAI API key not configured. Filtering and review will be disabled.")

# Initialize S3 downloader
try:
    aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
    aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
    s3_bucket = os.getenv('S3_BUCKET_NAME', 'toxicology-research-papers')
    
    if aws_access_key and aws_secret_key:
        s3_downloader = S3PaperDownloader(
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            bucket_name=s3_bucket,
            region_name='us-east-1'
        )
        logger.info(f"S3 downloader initialized for bucket: {s3_bucket}")
    else:
        logger.warning("S3 credentials not configured. Paper downloading to S3 will be disabled.")
except Exception as e:
    logger.error(f"Failed to initialize S3 downloader: {e}")

@app.on_event("startup")
async def startup_event():
    """Startup event to validate configuration"""
    if not apify_token:
        logger.error("API started without valid Apify token")
    if not openai_api_key:
        logger.error("API started without OpenAI API key - filtering disabled")
    if not s3_downloader:
        logger.warning("S3 integration not available - set AWS credentials to enable")
    else:
        logger.info("Fully Optimized Enhanced Toxicology API v7.1 started successfully")
        logger.info("Features: Concurrent scraping, concurrent AI processing, exponential backoff, S3 storage")

@app.post("/search", response_model=CompoundResponse)
async def search_compound(request: CompoundRequest):
    """
    Fully optimized search for toxicology papers with concurrent processing and S3 storage
    
    This endpoint:
    1. Validates the input compound
    2. Retrieves synonyms from PubChem
    3. Performs concurrent searches on Google Scholar and/or Europe PMC
    4. Extracts content using concurrent processing
    5. Filters articles using concurrent OpenAI processing
    6. Reviews articles with concurrent toxicological analysis
    7. Saves all papers to S3 with comprehensive metadata (if enabled)
    8. Returns results with detailed performance metrics
    
    Args:
        request: CompoundRequest with all optimization parameters
        
    Returns:
        CompoundResponse with filtered search results, performance metrics, and S3 save results
    """
    start_time = time.time()
    performance_metrics = {}
    
    try:
        # Validate input
        if not request.compound_name and not request.cas_number:
            raise HTTPException(
                status_code=400, 
                detail="At least one of compound_name or cas_number must be provided"
            )
        
        # Check if scrapers are initialized
        if not google_scholar_scraper or not europe_pmc_scraper:
            raise HTTPException(
                status_code=500,
                detail="Scrapers not configured. Please set APIFY_TOKEN in config."
            )
        
        # Dynamic initialization of scrapers with custom worker counts
        current_gs_scraper = google_scholar_scraper
        current_epmc_scraper = europe_pmc_scraper
        
        if request.google_scholar_workers != 15:
            current_gs_scraper = GoogleScholarScraper(apify_token, max_workers=request.google_scholar_workers)
            logger.info(f"Created custom Google Scholar scraper with {request.google_scholar_workers} workers")
        
        if request.europe_pmc_workers != 15:
            current_epmc_scraper = EuropePMCScraper(apify_token, max_workers=request.europe_pmc_workers)
            logger.info(f"Created custom Europe PMC scraper with {request.europe_pmc_workers} workers")
        
        # Dynamic initialization of filter and reviewer with custom worker counts
        current_filter = article_filter
        current_reviewer = article_reviewer
        
        if request.enable_filtering and openai_api_key:
            if request.filter_max_workers != 15:
                current_filter = ArticleFilter(openai_api_key, max_workers=request.filter_max_workers)
                logger.info(f"Created custom filter with {request.filter_max_workers} workers")
        
        if request.enable_toxicology_review and openai_api_key:
            if request.review_max_workers != 10:
                current_reviewer = ArticleReviewer(openai_api_key, max_workers=request.review_max_workers)
                logger.info(f"Created custom reviewer with {request.review_max_workers} workers")
        
        # Check filtering and review availability
        if request.enable_filtering and not current_filter:
            raise HTTPException(
                status_code=500,
                detail="Article filtering requested but OpenAI API key not configured."
            )
        
        if request.enable_toxicology_review and not current_reviewer:
            raise HTTPException(
                status_code=500,
                detail="Toxicology review requested but OpenAI API key not configured."
            )
        
        # Validate search sources
        valid_sources = ["google_scholar", "europe_pmc"]
        search_sources = [src for src in request.search_sources if src in valid_sources]
        
        if not search_sources:
            search_sources = valid_sources
            
        logger.info(f"Processing request: {request.compound_name}, CAS: {request.cas_number}")
        logger.info(f"Searching sources: {search_sources}")
        logger.info(f"Scraper workers - Google Scholar: {request.google_scholar_workers}, Europe PMC: {request.europe_pmc_workers}")
        logger.info(f"Filtering enabled: {request.enable_filtering} (workers: {request.filter_max_workers})")
        logger.info(f"Toxicology review enabled: {request.enable_toxicology_review} (workers: {request.review_max_workers})")
        logger.info(f"S3 save enabled: {request.save_to_s3}")
        
        # Step 1: Validate compound and get synonyms
        compound_start = time.time()
        compound_input = CompoundInput(
            name=request.compound_name or "",
            cas_number=request.cas_number or ""
        )
        
        validated_compound = await process_compound_input(compound_input)
        performance_metrics["compound_validation_time"] = time.time() - compound_start
        
        # Step 2: Create CompoundInfo for search
        compound_info = CompoundInfo(
            name=validated_compound.name,
            cas_number=validated_compound.cas_number,
            synonyms=validated_compound.synonyms
        )
        
        # Step 3: Execute optimized searches in parallel for selected sources
        search_start = time.time()
        search_tasks = []
        
        if "google_scholar" in search_sources:
            search_tasks.append(current_gs_scraper.search_compound(
                compound_info, 
                max_results_per_query=request.max_results_per_query
            ))
        
        if "europe_pmc" in search_sources:
            search_tasks.append(current_epmc_scraper.search_compound(
                compound_info,
                max_results_per_query=request.max_results_per_query
            ))
        
        # Wait for all search tasks to complete
        search_results_list = await asyncio.gather(*search_tasks)
        performance_metrics["search_time"] = time.time() - search_start
        
        # Step 4: Combine and annotate results with source
        combine_start = time.time()
        all_results = []
        google_scholar_results = []
        europe_pmc_results = []
        
        # Track which index corresponds to which source
        source_index_map = {}
        idx = 0
        if "google_scholar" in search_sources:
            source_index_map["google_scholar"] = idx
            idx += 1
        if "europe_pmc" in search_sources:
            source_index_map["europe_pmc"] = idx
        
        # Process Google Scholar results
        if "google_scholar" in search_sources:
            gs_results = search_results_list[source_index_map["google_scholar"]]
            google_scholar_results = gs_results
            for result in gs_results:
                result.source = "google_scholar"
                all_results.append(result)
                
        # Process Europe PMC results
        if "europe_pmc" in search_sources:
            epmc_results = search_results_list[source_index_map["europe_pmc"]]
            europe_pmc_results = epmc_results
            for result in epmc_results:
                result.source = "europe_pmc"
                all_results.append(result)
        
        performance_metrics["combine_time"] = time.time() - combine_start
        
        # Step 5: Deduplicate results based on title similarity and DOI
        dedup_start = time.time()
        unique_results = deduplicate_results_with_doi(all_results)
        performance_metrics["deduplication_time"] = time.time() - dedup_start
        performance_metrics["articles_before_dedup"] = len(all_results)
        performance_metrics["articles_after_dedup"] = len(unique_results)
        
        # Step 6: Apply OpenAI filtering if enabled (optimized version)
        filtering_summary = None
        excluded_articles = None
        
        if request.enable_filtering and current_filter:
            filter_start = time.time()
            logger.info(f"Starting optimized OpenAI filtering for {len(unique_results)} articles")
            
            filtering_result = await current_filter.filter_articles_batch_optimized(
                compound_name=validated_compound.name,
                articles=unique_results,
                min_confidence=request.min_confidence
            )
            
            unique_results = filtering_result["included_articles"]
            excluded_articles = filtering_result["excluded_articles"]
            filtering_summary = filtering_result["filtering_summary"]
            filtering_summary["study_types_distribution"] = filtering_result["study_types_distribution"]
            filtering_summary["exclusion_reasons_distribution"] = filtering_result["exclusion_reasons_distribution"]
            
            performance_metrics["filtering_time"] = time.time() - filter_start
            performance_metrics["filtering_rate"] = filtering_summary.get("articles_per_second", 0)
            
            logger.info(f"Optimized filtering complete: {len(unique_results)} articles in {filtering_summary['processing_time_seconds']:.2f}s")
        
        # Step 7: Apply comprehensive toxicology review if enabled (optimized version)
        toxicology_review_summary = None
        
        if request.enable_toxicology_review and current_reviewer and unique_results:
            review_start = time.time()
            logger.info(f"Starting optimized toxicology review for {len(unique_results)} articles")
            
            review_result = await current_reviewer.review_articles_batch_optimized(
                compound_name=validated_compound.name,
                articles=unique_results
            )
            
            unique_results = review_result["reviewed_articles"]
            toxicology_review_summary = review_result["review_summary"]
            
            performance_metrics["review_time"] = time.time() - review_start
            performance_metrics["review_rate"] = toxicology_review_summary.get("articles_per_second", 0)
            
            logger.info(f"Optimized review complete: {len(unique_results)} articles in {toxicology_review_summary['processing_time_seconds']:.2f}s")
        
        # Step 8: Get aggregated statistics
        stats_start = time.time()
        stats = get_combined_stats(google_scholar_results, europe_pmc_results, search_sources, current_gs_scraper, current_epmc_scraper)
        
        # Add filtering statistics
        if filtering_summary:
            stats["filtering"] = filtering_summary
        
        # Add toxicology review statistics
        if toxicology_review_summary:
            stats["toxicology_review"] = toxicology_review_summary
        
        performance_metrics["statistics_time"] = time.time() - stats_start
        
        # Calculate execution time
        execution_time = time.time() - start_time
        performance_metrics["total_execution_time"] = execution_time
        
        # Calculate optimization metrics
        performance_metrics["optimization_summary"] = {
            "total_articles_processed": len(all_results),
            "final_articles_returned": len(unique_results),
            "processing_rate_articles_per_second": len(all_results) / execution_time if execution_time > 0 else 0,
            "filtering_enabled": request.enable_filtering,
            "review_enabled": request.enable_toxicology_review,
            "s3_save_enabled": request.save_to_s3,
            "filter_workers": request.filter_max_workers if request.enable_filtering else 0,
            "review_workers": request.review_max_workers if request.enable_toxicology_review else 0,
            "scraper_workers": {
                "google_scholar": request.google_scholar_workers,
                "europe_pmc": request.europe_pmc_workers
            }
        }
        
        # Format response
        response_data = {
            "success": True,
            "compound_info": {
                "name": validated_compound.name,
                "cas_number": validated_compound.cas_number,
                "synonyms": validated_compound.synonyms,
                "iupac_name": validated_compound.iupac_name,
                "molecular_formula": validated_compound.molecular_formula,
                "molecular_weight": validated_compound.molecular_weight,
                "pubchem_cid": validated_compound.pubchem_cid
            },
            "search_results": [
                {
                    "title": result.title,
                    "authors": result.authors,
                    "abstract": result.abstract,
                    "link": result.link,
                    "pdf_link": result.pdf_link,
                    "citation_count": result.citation_count,
                    "year": result.year,
                    "search_term": result.search_term,
                    "full_text": result.full_text,
                    "content_extracted": result.content_extracted,
                    "extraction_method": result.extraction_method,
                    "extraction_scraper": result.extraction_scraper,
                    "text_length": len(result.full_text) if result.full_text else 0,
                    "source": result.source,
                    "doi": getattr(result, 'doi', None),
                    # Filtering information
                    "filtering_confidence": getattr(result, 'filtering_confidence', None),
                    "study_type": getattr(result, 'study_type', None),
                    "relevant_keywords": getattr(result, 'relevant_keywords', None),
                    "filtering_reasoning": getattr(result, 'filtering_result', {}).get('reasoning', None) if hasattr(result, 'filtering_result') else None,
                    # Toxicology review information
                    "toxicology_review": getattr(result, 'toxicology_review', None),
                    "toxicology_endpoints": getattr(result, 'toxicology_review', {}).get('toxicology_endpoints', None) if hasattr(result, 'toxicology_review') else None,
                    "study_design": getattr(result, 'toxicology_review', {}).get('study_design', None) if hasattr(result, 'toxicology_review') else None,
                    "key_findings": getattr(result, 'toxicology_review', {}).get('key_findings', None) if hasattr(result, 'toxicology_review') else None
                }
                for result in unique_results
            ],
            "excluded_articles": [
                {
                    "title": article.title,
                    "link": article.link,
                    "exclusion_reasons": getattr(article, 'exclusion_reasons', []),
                    "filtering_confidence": getattr(article, 'filtering_result', {}).get('confidence', 0.0) if hasattr(article, 'filtering_result') else 0.0,
                    "reasoning": getattr(article, 'filtering_result', {}).get('reasoning', '') if hasattr(article, 'filtering_result') else ''
                }
                for article in (excluded_articles or [])
            ] if excluded_articles else None,
            "filtering_summary": filtering_summary,
            "toxicology_review_summary": toxicology_review_summary,
            "statistics": stats,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat(),
            "sources_searched": search_sources,
            "performance_metrics": performance_metrics
        }
        
        # Step 9: Save papers to S3 if enabled
        s3_save_results = None
        if request.save_to_s3 and s3_downloader:
            try:
                logger.info(f"Saving {len(unique_results)} papers to S3...")
                s3_save_start = time.time()
                
                # Create the complete response for S3 saving
                complete_response_for_s3 = {
                    "success": True,
                    "compound_info": response_data["compound_info"],
                    "search_results": response_data["search_results"],
                    "excluded_articles": response_data.get("excluded_articles"),
                    "filtering_summary": response_data.get("filtering_summary"),
                    "toxicology_review_summary": response_data.get("toxicology_review_summary"),
                    "statistics": response_data["statistics"],
                    "execution_time": response_data["execution_time"],
                    "timestamp": response_data["timestamp"],
                    "sources_searched": response_data["sources_searched"],
                    "performance_metrics": response_data["performance_metrics"]
                }
                
                # Save all papers to S3
                s3_save_results = await s3_downloader.save_all_papers_to_s3(complete_response_for_s3)
                performance_metrics["s3_save_time"] = time.time() - s3_save_start
                
                logger.info(f"S3 save complete: {s3_save_results['successful_saves']}/{s3_save_results['total_papers']} papers saved")
                
                # Add S3 URLs to each paper in the response
                for i, paper in enumerate(response_data["search_results"]):
                    if i < len(s3_save_results["save_details"]):
                        save_detail = s3_save_results["save_details"][i]
                        if save_detail["success"]:
                            paper["s3_markdown_url"] = save_detail["s3_url"]
                            paper["s3_json_url"] = save_detail["s3_url"].replace('.md', '.json')
                
            except Exception as e:
                logger.error(f"Failed to save papers to S3: {e}")
                s3_save_results = {
                    "success": False,
                    "error": str(e),
                    "total_papers": len(unique_results),
                    "successful_saves": 0,
                    "failed_saves": len(unique_results)
                }
        
        # Add S3 results to the response
        response_data["s3_save_results"] = s3_save_results
        
        # Update performance metrics
        performance_metrics["s3_enabled"] = request.save_to_s3 and s3_downloader is not None
        response_data["performance_metrics"] = performance_metrics
        
        logger.info(f"Successfully processed compound. Found {len(unique_results)} relevant papers in {execution_time:.2f}s")
        logger.info(f"Papers with extracted content: {len([r for r in unique_results if r.content_extracted])}")
        logger.info(f"Papers with DOI: {len([r for r in unique_results if hasattr(r, 'doi') and r.doi])}")
        if filtering_summary:
            logger.info(f"Filtering: {filtering_summary['inclusion_rate']:.1f}% inclusion, {performance_metrics['filtering_rate']:.1f} articles/sec")
        if toxicology_review_summary:
            logger.info(f"Review: {toxicology_review_summary['total_articles']} articles, {performance_metrics['review_rate']:.1f} articles/sec")
        if s3_save_results:
            logger.info(f"S3: {s3_save_results['successful_saves']}/{s3_save_results['total_papers']} papers saved successfully")
        
        return response_data
        
    except Exception as e:
        logger.error(f"Error processing request: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error processing compound: {str(e)}"
        )

def deduplicate_results_with_doi(results: List[SearchResult]) -> List[SearchResult]:
    """
    Strict deduplication with multiple validation layers for toxicology research papers
    """
    if not results:
        return []
    
    import difflib
    import re
    
    def normalize_doi(doi: str) -> str:
        """Normalize DOI for comparison"""
        if not doi:
            return ""
        doi = doi.lower().strip()
        doi = re.sub(r'^(https?://)?(dx\.)?doi\.org/', '', doi)
        doi = re.sub(r'^(doi:)?', '', doi)
        return doi.strip()
    
    def normalize_text(text: str) -> str:
        """Normalize text for comparison"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def title_similarity(title1: str, title2: str) -> float:
        """Calculate similarity between two titles"""
        norm_title1 = normalize_text(title1)
        norm_title2 = normalize_text(title2)
        return difflib.SequenceMatcher(None, norm_title1, norm_title2).ratio()
    
    def author_overlap(authors1: List[str], authors2: List[str]) -> float:
        """Calculate percentage of overlapping authors"""
        if not authors1 or not authors2:
            return 0.0
        
        norm_authors1 = {normalize_text(author) for author in authors1}
        norm_authors2 = {normalize_text(author) for author in authors2}
        
        intersection = norm_authors1.intersection(norm_authors2)
        union = norm_authors1.union(norm_authors2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def are_duplicates(result1: SearchResult, result2: SearchResult) -> bool:
        """Comprehensive duplicate check with multiple criteria"""
        
        # Check DOI match (most reliable)
        doi1 = normalize_doi(getattr(result1, 'doi', ''))
        doi2 = normalize_doi(getattr(result2, 'doi', ''))
        if doi1 and doi2 and doi1 == doi2:
            return True
        
        # Check exact URL match
        if result1.link and result2.link:
            url1 = result1.link.lower().strip().rstrip('/')
            url2 = result2.link.lower().strip().rstrip('/')
            if url1 == url2:
                return True
        
        # Check title similarity with stricter threshold
        title_sim = title_similarity(result1.title, result2.title)
        
        if title_sim > 0.95:
            return True
        elif title_sim > 0.85:
            author_sim = author_overlap(result1.authors, result2.authors)
            if author_sim > 0.3:
                return True
            if abs(result1.year - result2.year) <= 1:
                return title_sim > 0.90
        
        return False
    
    def merge_results(primary: SearchResult, secondary: SearchResult) -> SearchResult:
        """Merge information from secondary result into primary result"""
        if not primary.full_text and secondary.full_text:
            primary.full_text = secondary.full_text
            primary.content_extracted = True
            primary.extraction_method = secondary.extraction_method
            primary.extraction_scraper = secondary.extraction_scraper
        
        primary_doi = normalize_doi(getattr(primary, 'doi', ''))
        secondary_doi = normalize_doi(getattr(secondary, 'doi', ''))
        
        if not primary_doi and secondary_doi:
            primary.doi = secondary.doi
        elif primary_doi and secondary_doi and len(secondary.doi) > len(getattr(primary, 'doi', '')):
            primary.doi = secondary.doi
        
        if secondary.abstract and len(secondary.abstract) > len(primary.abstract):
            primary.abstract = secondary.abstract
        
        if secondary.citation_count > primary.citation_count:
            primary.citation_count = secondary.citation_count
        
        if not primary.pdf_link and secondary.pdf_link:
            primary.pdf_link = secondary.pdf_link
        
        # Combine authors intelligently
        all_authors = primary.authors + secondary.authors
        seen_authors = set()
        unique_authors = []
        for author in all_authors:
            normalized_author = normalize_text(author)
            if normalized_author not in seen_authors:
                seen_authors.add(normalized_author)
                unique_authors.append(author)
        primary.authors = unique_authors[:15]
        
        if abs(primary.year - secondary.year) > 0:
            primary.year = max(primary.year, secondary.year)
        
        return primary
    
    # Sort results by quality
    sorted_results = sorted(results, key=lambda r: (
        r.content_extracted,
        bool(getattr(r, 'doi', None)),
        len(r.abstract),
        r.citation_count,
        r.year
    ), reverse=True)
    
    # Main deduplication logic
    unique_results = []
    result_clusters = []
    
    for result in sorted_results:
        merged_into_cluster = False
        
        for cluster_idx, cluster in enumerate(result_clusters):
            representative = cluster[0]
            
            if are_duplicates(result, representative):
                result_clusters[cluster_idx].append(result)
                merged_into_cluster = True
                break
        
        if not merged_into_cluster:
            result_clusters.append([result])
    
    # Process clusters to create final results
    for cluster in result_clusters:
        if not cluster:
            continue
        
        final_result = cluster[0]
        
        for other_result in cluster[1:]:
            final_result = merge_results(final_result, other_result)
        
        # Add source tracking
        sources = set()
        for result in cluster:
            if hasattr(result, 'source'):
                sources.add(result.source)
        
        if len(sources) > 1:
            final_result.source = "multiple"
            if not hasattr(final_result, 'sources_list'):
                final_result.sources_list = list(sources)
        
        unique_results.append(final_result)
    
    logger.info(f"Strict deduplication: {len(results)} -> {len(unique_results)} unique papers")
    logger.info(f"Found {len(results) - len(unique_results)} duplicates across sources")
    
    unique_results.sort(key=lambda r: (
        r.content_extracted,
        r.citation_count,
        r.year
    ), reverse=True)
    
    return unique_results

def get_combined_stats(google_scholar_results: List[SearchResult], 
                       europe_pmc_results: List[SearchResult],
                       sources_searched: List[str],
                       current_gs_scraper=None,
                       current_epmc_scraper=None) -> Dict:
    """Get combined statistics from multiple search sources"""
    stats = {
        "total_papers": 0,
        "papers_with_full_text": 0,
        "papers_with_abstract_only": 0,
        "papers_with_doi": 0,
        "extraction_success_rate": 0,
        "sources": {}
    }
    
    # Add Google Scholar statistics if available
    if "google_scholar" in sources_searched and google_scholar_results:
        scraper = current_gs_scraper if current_gs_scraper else google_scholar_scraper
        gs_stats = scraper.get_summary_stats(google_scholar_results)
        doi_count = sum(1 for r in google_scholar_results if hasattr(r, 'doi') and r.doi)
        gs_stats["papers_with_doi"] = doi_count
        stats["sources"]["google_scholar"] = gs_stats
        stats["total_papers"] += gs_stats.get("total_papers", 0)
        stats["papers_with_full_text"] += gs_stats.get("papers_with_full_text", 0)
        stats["papers_with_abstract_only"] += gs_stats.get("papers_with_abstract_only", 0)
        stats["papers_with_doi"] += doi_count
    
    # Add Europe PMC statistics if available
    if "europe_pmc" in sources_searched and europe_pmc_results:
        scraper = current_epmc_scraper if current_epmc_scraper else europe_pmc_scraper
        epmc_stats = scraper.get_summary_stats(europe_pmc_results)
        stats["sources"]["europe_pmc"] = epmc_stats
        stats["total_papers"] += epmc_stats.get("total_papers", 0)
        stats["papers_with_full_text"] += epmc_stats.get("papers_with_full_text", 0)
        stats["papers_with_abstract_only"] += epmc_stats.get("papers_with_abstract_only", 0)
        stats["papers_with_doi"] += epmc_stats.get("papers_with_doi", 0)
    
    # Calculate overall extraction success rate
    total_papers = stats["total_papers"]
    if total_papers > 0:
        stats["extraction_success_rate"] = (stats["papers_with_full_text"] / total_papers) * 100
    
    return stats

# S3 Management Endpoints
@app.get("/s3/list-papers")
async def list_s3_papers(compound_name: Optional[str] = None):
    """List all saved papers in S3, optionally filtered by compound"""
    if not s3_downloader:
        raise HTTPException(
            status_code=503,
            detail="S3 downloader not configured. Please set AWS credentials."
        )
    
    try:
        papers = s3_downloader.list_saved_papers(compound_name)
        return {
            "success": True,
            "compound_filter": compound_name,
            "total_papers": len(papers),
            "papers": papers,
            "s3_bucket": s3_downloader.bucket_name
        }
    except Exception as e:
        logger.error(f"Error listing S3 papers: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error listing papers: {str(e)}"
        )

@app.get("/s3/get-paper")
async def get_s3_paper(s3_key: str):
    """Retrieve paper content from S3"""
    if not s3_downloader:
        raise HTTPException(
            status_code=503,
            detail="S3 downloader not configured. Please set AWS credentials."
        )
    
    try:
        content = s3_downloader.get_paper_content(s3_key)
        if content is None:
            raise HTTPException(
                status_code=404,
                detail="Paper not found in S3"
            )
        
        return {
            "success": True,
            "s3_key": s3_key,
            "content": content,
            "s3_bucket": s3_downloader.bucket_name
        }
    except Exception as e:
        logger.error(f"Error retrieving S3 paper: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error retrieving paper: {str(e)}"
        )

@app.post("/s3/save-papers")
async def save_existing_papers_to_s3(api_response: Dict):
    """Save previously retrieved papers to S3"""
    if not s3_downloader:
        raise HTTPException(
            status_code=503,
            detail="S3 downloader not configured. Please set AWS credentials."
        )
    
    try:
        s3_save_results = await s3_downloader.save_all_papers_to_s3(api_response)
        return {
            "success": True,
            "s3_save_results": s3_save_results,
            "s3_bucket": s3_downloader.bucket_name
        }
    except Exception as e:
        logger.error(f"Error saving papers to S3: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error saving papers: {str(e)}"
        )

@app.get("/health")
async def health_check():
    """Health check endpoint with full optimization and S3 status"""
    apify_configured = apify_token is not None
    openai_configured = openai_api_key is not None
    s3_configured = s3_downloader is not None and s3_downloader.s3_client is not None
    
    gs_status = "initialized" if google_scholar_scraper else "not_initialized"
    epmc_status = "initialized" if europe_pmc_scraper else "not_initialized"
    filter_status = "initialized" if article_filter else "not_initialized"
    reviewer_status = "initialized" if article_reviewer else "not_initialized"
    s3_status = "initialized" if s3_configured else "not_initialized"
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "7.1.0",
        "optimization_status": "fully_enabled",
        "apify_configured": apify_configured,
        "openai_configured": openai_configured,
        "s3_configured": s3_configured,
        "scrapers": {
            "google_scholar": {
                "status": gs_status,
                "concurrent_workers": 15,
                "optimization": "Concurrent PDF and HTML extraction"
            },
            "europe_pmc": {
                "status": epmc_status,
                "concurrent_workers": 15,
                "optimization": "Concurrent full-text XML extraction"
            }
        },
        "filtering": {
            "status": filter_status,
            "model": "gpt-4.1-mini-2025-04-14" if article_filter else None,
            "enabled": openai_configured,
            "concurrent_workers": 15 if article_filter else 0,
            "optimization": "ThreadPoolExecutor with backoff"
        },
        "toxicology_review": {
            "status": reviewer_status,
            "model": "gpt-4.1-mini-2025-04-14" if article_reviewer else None,
            "enabled": openai_configured,
            "concurrent_workers": 10 if article_reviewer else 0,
            "optimization": "ThreadPoolExecutor with backoff"
        },
        "s3_storage": {
            "status": s3_status,
            "bucket": s3_downloader.bucket_name if s3_configured else None,
            "region": s3_downloader.region_name if s3_configured else None,
            "features": [
                "Automatic paper saving to S3",
                "Markdown and JSON format support",
                "Hierarchical folder structure",
                "Comprehensive metadata storage",
                "Batch save summaries"
            ] if s3_configured else []
        },
        "actors": {
            "scholar_search": "marco.gullo/google-scholar-scraper",
            "pdf_scraper": "jirimoravcik/pdf-text-extractor",
            "cheerio_scraper": "apify/cheerio-scraper",
            "note": "Europe PMC uses REST API only, no Apify actors needed"
        }
    }

@app.get("/")
async def root():
    """Root endpoint with complete optimization and S3 information"""
    return {
        "message": "Fully Optimized Enhanced Toxicology Research API v7.1 with S3 Storage",
        "version": "7.1.0",
        "optimization_status": "Full concurrent processing enabled",
        "performance_improvements": {
            "concurrent_scrapers": "Optimized Google Scholar and Europe PMC scrapers with concurrent extraction",
            "concurrent_filtering": "Up to 15 concurrent workers for article filtering",
            "concurrent_review": "Up to 10 concurrent workers for toxicology review",
            "exponential_backoff": "Robust retry logic for all API calls",
            "optimized_prompts": "Reduced token usage for faster processing",
            "lambda_ready": "Optimized for AWS Lambda deployment",
            "s3_integration": "Automatic paper storage with comprehensive metadata",
            "typical_speedup": "15x-25x faster than sequential processing"
        },
        "new_features": {
            "optimized_scrapers": "High-performance Google Scholar and Europe PMC scrapers with concurrent processing",
            "configurable_scraper_workers": "Adjust concurrent worker counts for optimal scraper performance",
            "optimized_ai_processing": "High-performance filtering and review with ThreadPoolExecutor",
            "exponential_backoff": "Robust retry logic with backoff for all API calls",
            "concurrent_processing": "Process multiple operations simultaneously for faster results",
            "configurable_workers": "Adjust concurrent worker counts for optimal performance",
            "performance_metrics": "Detailed timing and rate metrics for each processing stage",
            "ai_filtering": "OpenAI GPT-4.1-mini based article filtering for toxicology research",
            "filtering_confidence": "Configurable confidence thresholds for article inclusion",
            "study_type_classification": "Automatic classification of study types",
            "detailed_exclusion_reasons": "Detailed reasons for article exclusions",
            "toxicology_review": "Comprehensive toxicological analysis with endpoints and study design",
            "endpoints_extraction": "Automatic extraction of NOAEL, LOAEL, EC50, LD50, etc.",
            "study_design_analysis": "Detailed analysis of species, routes, and study duration",
            "paywall_detection": "Detection of full text availability vs abstract only",
            "regulatory_insights": "Cramer class and regulatory guideline identification",
            "europe_pmc_integration": "Optimized Europe PMC with concurrent XML extraction",
            "doi_extraction": "Extract and track DOIs from papers",
            "strict_deduplication": "Enhanced deduplication using DOI, title similarity, author overlap, and year proximity",
            "s3_storage": "Automatic saving of papers to S3 with rich metadata",
            "dual_format_storage": "Papers saved in both markdown and JSON formats",
            "hierarchical_organization": "Organized by compound name with timestamps"
        },
        "endpoints": {
            "search": "/search (POST) - Optimized search with concurrent processing, AI analysis, and S3 storage",
            "health": "/health (GET) - Health check with optimization and S3 status",
            "docs": "/docs - API documentation",
            "test": "/test/{compound_name} - Quick test with all optimizations",
            "filter-test": "/filter-test/{compound_name} - Test optimized filtering",
            "review-test": "/review-test/{compound_name} - Test optimized toxicology review",
            "performance": "/performance-test/{compound_name} - Performance benchmarking",
            "source_stats": "/source-stats/{compound_name} - Source-specific statistics",
            "s3_list": "/s3/list-papers - List all saved papers in S3",
            "s3_get": "/s3/get-paper - Retrieve specific paper content from S3",
            "s3_save": "/s3/save-papers - Save existing search results to S3"
        },
        "s3_features": {
            "automatic_save": "Automatically save filtered papers to S3 bucket",
            "comprehensive_metadata": "Full paper metadata with toxicology analysis",
            "multiple_formats": "Both markdown and JSON formats supported",
            "hierarchical_storage": "Organized by compound name with timestamps",
            "batch_summaries": "Summary reports for each search batch",
            "retrieval_endpoints": "API endpoints to list and retrieve saved papers"
        },
        "filtering_criteria": {
            "included_studies": [
                "Inhalation toxicity studies",
                "Animal studies (any route)",
                "In vitro cell-based studies", 
                "Thermal stability & pyrolysis studies",
                "In silico/QSAR modeling"
            ],
            "excluded_studies": [
                "Reference-only mentions",
                "Chemical composition analysis only",
                "Insecticidal/repellent studies",
                "Chemical synthesis focus",
                "Degradation product mentions only"
            ]
        },
        "search_strategy": {
            "google_scholar": "Optimized PDF-first extraction with concurrent Cheerio fallback for HTML content",
            "europe_pmc": "Optimized REST API with concurrent XML full-text extraction"
        },
        "ai_model": "GPT-4.1-mini (gpt-4.1-mini-2025-04-14)",
        "deployment": {
            "lambda_optimized": True,
            "typical_processing_time": "1-2 minutes for 100+ articles",
            "scalability": "Handles 200+ articles efficiently with concurrent processing",
            "s3_configuration": {
                "required_env_vars": [
                    "AWS_ACCESS_KEY_ID",
                    "AWS_SECRET_ACCESS_KEY", 
                    "S3_BUCKET_NAME (optional, defaults to 'toxicology-research-papers')"
                ],
                "file_structure": "s3://bucket/toxicology_research/{compound_name}/{timestamp}_{paper_title}.md"
            }
        }
    }

@app.get("/test/{compound_name}")
async def test_search(
    compound_name: str, 
    cas_number: Optional[str] = None,
    sources: Optional[str] = "google_scholar,europe_pmc",
    enable_filtering: Optional[bool] = True,
    enable_toxicology_review: Optional[bool] = True,
    save_to_s3: Optional[bool] = True,
    min_confidence: Optional[float] = 0.7,
    filter_workers: Optional[int] = 15,
    review_workers: Optional[int] = 10,
    scraper_workers: Optional[int] = 15
):
    """
    Test endpoint for quick searches with optimized filtering, toxicology review, scrapers, and S3 storage
    Usage: GET /test/limonene?cas_number=5989-27-5&sources=google_scholar,europe_pmc&enable_filtering=true&enable_toxicology_review=true&save_to_s3=true&filter_workers=15&review_workers=10&scraper_workers=15
    """
    search_sources = sources.split(",") if sources else ["google_scholar", "europe_pmc"]
    
    request = CompoundRequest(
        compound_name=compound_name, 
        cas_number=cas_number,
        search_sources=search_sources,
        max_results_per_query=3,  # Limit results for quick testing
        enable_filtering=enable_filtering,
        enable_toxicology_review=enable_toxicology_review,
        save_to_s3=save_to_s3,
        min_confidence=min_confidence,
        filter_max_workers=filter_workers,
        review_max_workers=review_workers,
        google_scholar_workers=scraper_workers,
        europe_pmc_workers=scraper_workers
    )
    
    return await search_compound(request)

@app.get("/filter-test/{compound_name}")
async def test_filtering_only(
    compound_name: str,
    max_results: Optional[int] = 3
):
    """Test endpoint to demonstrate optimized filtering capabilities"""
    try:
        if not article_filter:
            return {"error": "Article filter not initialized - OpenAI API key required"}
        
        # Create sample articles for testing
        sample_articles = [
            SearchResult(
                title="Inhalation toxicity of limonene in Sprague-Dawley rats",
                authors=["Smith J", "Doe A"],
                abstract="This study investigated acute and subchronic inhalation toxicity of d-limonene vapor in Sprague-Dawley rats. Animals were exposed to 0, 300, 1000, or 3000 ppm d-limonene vapor for 6 hours/day, 5 days/week for 4 weeks.",
                link="https://example.com/paper1",
                pdf_link=None,
                citation_count=25,
                year=2023,
                search_term=compound_name
            ),
            SearchResult(
                title="Chemical synthesis of limonene from citrus waste",
                authors=["Johnson B"],
                abstract="This paper describes a novel method for chemical synthesis of d-limonene from citrus peel waste using green chemistry principles.",
                link="https://example.com/paper2",
                pdf_link=None,
                citation_count=15,
                year=2022,
                search_term=compound_name
            ),
            SearchResult(
                title="In vitro cytotoxicity of limonene on human lung cells",
                authors=["Brown C", "Wilson D"],
                abstract="We evaluated the cytotoxic effects of limonene on A549 human lung adenocarcinoma cells using MTT assay and flow cytometry. Cells were exposed to concentrations ranging from 10-1000 Î¼M.",
                link="https://example.com/paper3",
                pdf_link=None,
                citation_count=18,
                year=2023,
                search_term=compound_name
            )
        ]
        
        # Test optimized filtering
        filtering_result = await article_filter.filter_articles_batch_optimized(
            compound_name=compound_name,
            articles=sample_articles[:max_results],
            min_confidence=0.5
        )
        
        return {
            "compound": compound_name,
            "model_used": "gpt-4.1-mini-2025-04-14",
            "optimization": "ThreadPoolExecutor with 15 workers",
            "filtering_summary": filtering_result["filtering_summary"],
            "included_articles": [
                {
                    "title": article.title,
                    "confidence": article.filtering_confidence,
                    "study_type": article.study_type,
                    "relevant_keywords": article.relevant_keywords,
                    "reasoning": article.filtering_result.get("reasoning", "")
                }
                for article in filtering_result["included_articles"]
            ],
            "excluded_articles": [
                {
                    "title": article.title,
                    "exclusion_reasons": article.exclusion_reasons,
                    "reasoning": article.filtering_result.get("reasoning", "")
                }
                for article in filtering_result["excluded_articles"]
            ]
        }
        
    except Exception as e:
        logger.error(f"Error in optimized filter test: {e}")
        return {"error": str(e)}

@app.get("/performance-test/{compound_name}")
async def performance_benchmark(
    compound_name: str,
    max_results: Optional[int] = 10,
    filter_workers: Optional[int] = 15,
    review_workers: Optional[int] = 10,
    scraper_workers: Optional[int] = 15,
    save_to_s3: Optional[bool] = True
):
    """Performance benchmark endpoint to test full optimization including S3"""
    try:
        start_time = time.time()
        
        # Run a complete search with optimized settings
        request = CompoundRequest(
            compound_name=compound_name,
            search_sources=["google_scholar", "europe_pmc"],
            max_results_per_query=max_results,
            enable_filtering=True,
            enable_toxicology_review=True,
            save_to_s3=save_to_s3,
            min_confidence=0.7,
            filter_max_workers=filter_workers,
            review_max_workers=review_workers,
            google_scholar_workers=scraper_workers,
            europe_pmc_workers=scraper_workers
        )
        
        response = await search_compound(request)
        total_time = time.time() - start_time
        
        # Extract performance metrics
        performance = response.performance_metrics
        
        # Calculate benchmark results
        benchmark_results = {
            "compound": compound_name,
            "optimization_settings": {
                "filter_workers": filter_workers,
                "review_workers": review_workers,
                "scraper_workers": scraper_workers,
                "s3_save_enabled": save_to_s3
            },
            "benchmark_summary": {
                "total_execution_time": total_time,
                "articles_processed": performance["articles_before_dedup"],
                "articles_after_dedup": performance["articles_after_dedup"],
                "articles_filtered": len(response.search_results),
                "processing_rate": performance["articles_before_dedup"] / total_time if total_time > 0 else 0
            },
            "stage_breakdown": {
                "compound_validation": performance.get("compound_validation_time", 0),
                "search_time": performance.get("search_time", 0),
                "deduplication_time": performance.get("deduplication_time", 0),
                "filtering_time": performance.get("filtering_time", 0),
                "review_time": performance.get("review_time", 0),
                "s3_save_time": performance.get("s3_save_time", 0)
            },
            "optimization_metrics": {
                "filtering_rate": performance.get("filtering_rate", 0),
                "review_rate": performance.get("review_rate", 0),
                "concurrent_scraping": scraper_workers,
                "concurrent_filtering": filter_workers,
                "concurrent_review": review_workers,
                "s3_enabled": performance.get("s3_enabled", False)
            },
            "comparison_to_sequential": {
                "estimated_sequential_time": (
                    performance["articles_before_dedup"] * 3.0 +  # Estimated 3s per article for scraping
                    performance["articles_before_dedup"] * 2.0 +  # Estimated 2s per article for filtering
                    len(response.search_results) * 5.0 +  # Estimated 5s per article for review
                    len(response.search_results) * 1.0  # Estimated 1s per article for S3 save
                ),
                "speedup_factor": 0  # Will be calculated below
            }
        }
        
        # Calculate speedup factor
        estimated_sequential = benchmark_results["comparison_to_sequential"]["estimated_sequential_time"]
        benchmark_results["comparison_to_sequential"]["speedup_factor"] = (
            estimated_sequential / total_time if total_time > 0 else 0
        )
        
        # Add filtering and review summaries
        if response.filtering_summary:
            benchmark_results["filtering_performance"] = response.filtering_summary
        
        if response.toxicology_review_summary:
            benchmark_results["review_performance"] = response.toxicology_review_summary
        
        # Add S3 results
        if response.s3_save_results:
            benchmark_results["s3_performance"] = {
                "papers_saved": response.s3_save_results.get("successful_saves", 0),
                "s3_save_rate": response.s3_save_results.get("successful_saves", 0) / performance.get("s3_save_time", 1) if performance.get("s3_save_time", 0) > 0 else 0,
                "s3_bucket": response.s3_save_results.get("s3_bucket", ""),
                "save_time": performance.get("s3_save_time", 0)
            }
        
        return benchmark_results
        
    except Exception as e:
        logger.error(f"Error in performance benchmark: {e}")
        return {"error": str(e)}

# Additional endpoints for testing individual components
@app.get("/source-stats/{compound_name}")
async def source_statistics(
    compound_name: str,
    sources: Optional[str] = "google_scholar,europe_pmc",
    enable_filtering: Optional[bool] = True,
    save_to_s3: Optional[bool] = False
):
    """Compare statistics across different sources with optimization metrics"""
    try:
        search_sources = sources.split(",") if sources else ["google_scholar", "europe_pmc"]
        
        request = CompoundRequest(
            compound_name=compound_name,
            search_sources=search_sources,
            max_results_per_query=10,
            enable_filtering=enable_filtering,
            save_to_s3=save_to_s3
        )
        
        response = await search_compound(request)
        
        # Analyze results by source with performance metrics
        source_analysis = {}
        
        for result in response.search_results:
            source = result.source
            if source not in source_analysis:
                source_analysis[source] = {
                    "total_papers": 0,
                    "papers_with_full_text": 0,
                    "papers_with_abstract_only": 0,
                    "papers_with_doi": 0,
                    "extraction_methods": {},
                    "study_types": {},
                    "avg_filtering_confidence": 0,
                    "total_confidence": 0,
                    "s3_saved": 0
                }
            
            stats = source_analysis[source]
            stats["total_papers"] += 1
            
            if result.full_text:
                stats["papers_with_full_text"] += 1
            elif result.abstract:
                stats["papers_with_abstract_only"] += 1
            
            if result.doi:
                stats["papers_with_doi"] += 1
            
            if result.extraction_method:
                stats["extraction_methods"][result.extraction_method] = stats["extraction_methods"].get(result.extraction_method, 0) + 1
            
            if result.study_type:
                stats["study_types"][result.study_type] = stats["study_types"].get(result.study_type, 0) + 1
            
            if result.filtering_confidence:
                stats["total_confidence"] += result.filtering_confidence
            
            if hasattr(result, 's3_markdown_url') and result.s3_markdown_url:
                stats["s3_saved"] += 1
        
        # Calculate averages
        for source, stats in source_analysis.items():
            if stats["total_papers"] > 0:
                stats["avg_filtering_confidence"] = stats["total_confidence"] / stats["total_papers"]
        
        return {
            "compound": compound_name,
            "sources_searched": search_sources,
            "filtering_enabled": enable_filtering,
            "s3_save_enabled": save_to_s3,
            "optimization_status": "fully_enabled",
            "performance_metrics": response.performance_metrics,
            "source_analysis": source_analysis,
            "source_comparison": {
                "papers_found": {source: stats["total_papers"] for source, stats in source_analysis.items()},
                "extraction_success_rate": {
                    source: (stats["papers_with_full_text"] / stats["total_papers"] * 100) if stats["total_papers"] > 0 else 0
                    for source, stats in source_analysis.items()
                },
                "doi_coverage": {
                    source: (stats["papers_with_doi"] / stats["total_papers"] * 100) if stats["total_papers"] > 0 else 0
                    for source, stats in source_analysis.items()
                },
                "avg_filtering_confidence": {source: stats["avg_filtering_confidence"] for source, stats in source_analysis.items()},
                "s3_save_rate": {
                    source: (stats["s3_saved"] / stats["total_papers"] * 100) if stats["total_papers"] > 0 else 0
                    for source, stats in source_analysis.items()
                }
            },
            "filtering_summary": response.filtering_summary,
            "s3_save_results": response.s3_save_results
        }
        
    except Exception as e:
        logger.error(f"Error generating source statistics: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    import uvicorn
    
    # Check configuration
    if not get_apify_token():
        print("WARNING: Apify token not configured!")
        print("Please set your token in config.py or as environment variable:")
        print("export APIFY_TOKEN='your_actual_token'")
        print("Get your token from: https://console.apify.com/")
    
    if not os.getenv("OPENAI_API_KEY"):
        print("WARNING: OpenAI API key not configured!")
        print("Please set OPENAI_API_KEY environment variable")
        print("Article filtering will be disabled without this key")
    
    if not s3_downloader:
        print("WARNING: S3 integration not configured!")
        print("Please set the following environment variables to enable S3 storage:")
        print("export AWS_ACCESS_KEY_ID='your_access_key'")
        print("export AWS_SECRET_ACCESS_KEY='your_secret_key'")
        print("export S3_BUCKET_NAME='your_bucket_name' (optional)")
    
    print("Starting Fully Optimized Enhanced Toxicology Research API v7.1...")
    print("Features: Concurrent scraping + concurrent AI processing + exponential backoff + S3 storage")
    print()
    print("API Documentation: http://localhost:8000/docs")
    print("Test endpoint: http://localhost:8000/test/limonene?scraper_workers=15&filter_workers=15&review_workers=10&save_to_s3=true")
    print("Performance test: http://localhost:8000/performance-test/limonene?save_to_s3=true")
    print("Filter test: http://localhost:8000/filter-test/limonene")
    print("Source stats: http://localhost:8000/source-stats/limonene")
    print("S3 list papers: http://localhost:8000/s3/list-papers")
    print()
    print("OPTIMIZATION STATUS: FULLY ENABLED")
    print("- Concurrent scraping with ThreadPoolExecutor")
    print("- Concurrent filtering with ThreadPoolExecutor")
    print("- Concurrent toxicology review with ThreadPoolExecutor")
    print("- Exponential backoff for API resilience")
    print("- Automatic S3 storage with comprehensive metadata")
    print("- Optimized for AWS Lambda deployment")
    print("- Expected speedup: 15x-25x over sequential processing")
    
    if s3_downloader:
        print(f"- S3 Storage: Enabled (Bucket: {s3_downloader.bucket_name})")
    else:
        print("- S3 Storage: Disabled (set AWS credentials to enable)")
    
    # Run the API server
    uvicorn.run(app, host="0.0.0.0", port=8000)
